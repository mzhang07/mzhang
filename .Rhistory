aux = mean((test[,2]-near$fitted)^2)
MSE = c(MSE,aux)
plot(lstat,medv,main=paste("k=",i),pch=19,cex=0.8,col="darkgray")
lines(test[,1],near$fitted,col=2,lwd=2)
cat ("Press [enter] to continue")
line <- readline()
}
plot(log(1/kk),sqrt(MSE),type="b",xlab="Complexity (log(1/k))",col="blue",ylab="RMSE",lwd=2,cex.lab=1.2)
text(log(1/kk[1]),sqrt(MSE[1])+0.3,paste("k=",kk[1]),col=2,cex=1.2)
text(log(1/kk[10])+0.4,sqrt(MSE[10]),paste("k=",kk[10]),col=2,cex=1.2)
text(log(1/kk[5])+0.4,sqrt(MSE[5]),paste("k=",kk[5]),col=2,cex=1.2)
near = kknn(medv~lstat,train,test,k=20,kernel = "rectangular")
for(i in seq(1,505,by=100)){
ii = near$C[i,1:20]
plot(lstat,medv,main=paste("k=",20),pch=19,cex=0.8,col="darkgray")
lines(test[,1],near$fitted,col=2,lwd=2)
abline(v=test[i,1],col=2,lty=2)
points(lstat[ii],medv[ii],pch=19,col="blue")
cat ("Press [enter] to continue")
line <- readline()
near = kknn(medv~lstat,train,test,k=20,kernel = "rectangular")
for(i in seq(1,505,by=100)){
ii = near$C[i,1:20]
plot(lstat,medv,main=paste("k=",20),pch=19,cex=0.8,col="darkgray")
lines(test[,1],near$fitted,col=2,lwd=2)
abline(v=test[i,1],col=2,lty=2)
points(lstat[ii],medv[ii],pch=19,col="blue")
cat ("Press [enter] to continue")
line <- readline()
}
train = data.frame(lstat,medv)
test = data.frame(lstat,medv)
tr = sample(1:506,400)
train = train[tr,]
test = test[-tr,]
out_MSE = NULL
for(i in 2:350){
near = kknn(medv~lstat,train,test,k=i,kernel = "rectangular")
aux = mean((test[,2]-near$fitted)^2)
out_MSE = c(out_MSE,aux)
}
best = which.min(out_MSE)
plot(log(1/(2:350)),sqrt(out_MSE),xlab="Complexity (log(1/k))",ylab="out-of-sample RMSE",col=4,lwd=2,type="l",cex.lab=1.2)
text(log(1/best),sqrt(out_MSE[best])+0.3,paste("k=",best),col=2,cex=1.2)
text(log(1/2),sqrt(out_MSE[2]),paste("k=",2),col=2,cex=1.2)
text(log(1/354)+0.4,sqrt(out_MSE[345]),paste("k=",345),col=2,cex=1.2)
near = kknn(medv~lstat,train,test,k=42,kernel = "rectangular")
ind = order(test[,1])
plot(lstat,medv,main=paste("k=",42),pch=19,cex=0.8,col="darkgray")
lines(test[ind,1],near$fitted[ind],col=2,lwd=2)
train = data.frame(lstat,medv)
test = data.frame(lstat,medv)
tr = sample(1:506,400) ## drawing sample for training set
train = train[tr,]
test = test[-tr,] ## remaining instances for test set
dim(train)
library(MASS) ## a library of example datasets
library(class) ## a library with lots of classification tools
library(kknn) ## kknn library
attach(Boston) ## load data file
n = dim(Boston)[1] ## takes in 1st record in dimensions, which is n (sample size)
plot(lstat,medv) ## plots lstat on x axis and medv on y axis
train = data.frame(lstat,medv) ## training dataset
test = data.frame(lstat,medv) ## test dataset, they are the same at this point
ind = order(test[,1]) ## ordering the test data frame for plotting in order
test = test[ind,]
MSE = NULL
kk = c(2,10,50,100,150,200,250,300,400,505) ## various k-nearest neighbors to try
for(i in kk){
near = kknn(medv~lstat,train,test,k=i,kernel = "rectangular") # calculating the k-nearest neighbors
aux = mean((test[,2]-near$fitted)^2) ## calculating mean squared error
MSE = c(MSE,aux) ## setting a list of MSEs for all the different k-nearest neighbors
## c function is concatenating function
plot(lstat,medv,main=paste("k=",i),pch=19,cex=0.8,col="darkgray")
lines(test[,1],near$fitted,col=2,lwd=2) ## connects points in sequence
cat ("Press [enter] to continue") ## break before the next iteration
line <- readline()
}
test_i = test[j,]
for(i in 1:100){
near = kknn(medv~lstat,train_i,test_i,k=i,kernel = "rectangular")
aux = mean((test_i[,2]-near$fitted)^2)
out_MSE[j,i] = aux
}
cat(j,'\n')
}
mMSE = apply(out_MSE,2,mean)
plot(log(1/(1:100)),sqrt(mMSE),xlab="Complexity (log(1/k))",ylab="out-of-sample RMSE",col=4,lwd=2,type="l",cex.lab=1.2)
best = which.min(mMSE)
text(log(1/best),sqrt(mMSE[best])+0.1,paste("k=",best),col=2,cex=1.2)
text(log(1/2),sqrt(mMSE[2])+0.3,paste("k=",2),col=2,cex=1.2)
text(log(1/100)+0.4,sqrt(mMSE[100]),paste("k=",100),col=2,cex=1.2)
out_MSE = matrix(0,n,100)
out_MSE
library(MASS) ## a library of example datasets
library(class) ## a library with lots of classification tools
library(kknn) ## kknn library
attach(Boston) ## load data file
n = dim(Boston)[1] ## takes in 1st record in dimensions, which is n (sample size)
plot(lstat,medv) ## plots lstat on x axis and medv on y axis
train = data.frame(lstat,medv) ## training dataset
test = data.frame(lstat,medv) ## test dataset, they are the same at this point
ind = order(test[,1]) ## ordering the test data frame for plotting in order
test = test[ind,]
MSE = NULL
kk = c(2,10,50,100,150,200,250,300,400,505) ## various k-nearest neighbors to try
for(i in kk){
near = kknn(medv~lstat,train,test,k=i,kernel = "rectangular") # calculating the k-nearest neighbors
aux = mean((test[,2]-near$fitted)^2) ## calculating mean squared error
MSE = c(MSE,aux) ## setting a list of MSEs for all the different k-nearest neighbors
## c function is concatenating function
plot(lstat,medv,main=paste("k=",i),pch=19,cex=0.8,col="darkgray")
lines(test[,1],near$fitted,col=2,lwd=2) ## connects points in sequence
cat ("Press [enter] to continue") ## break before the next iteration
line <- readline()
}
ii = near$C[i,1:20]
plot(lstat,medv,main=paste("k=",20),pch=19,cex=0.8,col="darkgray")
lines(test[,1],near$fitted,col=2,lwd=2)
abline(v=test[i,1],col=2,lty=2)
points(lstat[ii],medv[ii],pch=19,col="blue")
cat ("Press [enter] to continue")
line <- readline()
######################################
## OUT-OF-SAMPLE Prediction
######################################
train = data.frame(lstat,medv)
test = data.frame(lstat,medv)
tr = sample(1:506,400) ## drawing sample for training set
train = train[tr,]
test = test[-tr,] ## remaining instances for test set
out_MSE = NULL
for(i in 2:350){
near = kknn(medv~lstat,train,test,k=i,kernel = "rectangular")
aux = mean((test[,2]-near$fitted)^2)
out_MSE = c(out_MSE,aux)
}
## set the seed for replication
best = which.min(out_MSE) ## which position of the vector contains the minimum
plot(log(1/(2:350)),sqrt(out_MSE),xlab="Complexity (log(1/k))",ylab="out-of-sample RMSE",col=4,lwd=2,type="l",cex.lab=1.2)
text(log(1/best),sqrt(out_MSE[best])+0.3,paste("k=",best),col=2,cex=1.2)
text(log(1/2),sqrt(out_MSE[2]),paste("k=",2),col=2,cex=1.2)
text(log(1/354)+0.4,sqrt(out_MSE[345]),paste("k=",345),col=2,cex=1.2)
near = kknn(medv~lstat,train,test,k=42,kernel = "rectangular")
ind = order(test[,1])
plot(lstat,medv,main=paste("k=",42),pch=19,cex=0.8,col="darkgray")
lines(test[ind,1],near$fitted[ind],col=2,lwd=2)
#########################################
# leave-one-out cross validation (LOOCV)#
#########################################
train = data.frame(lstat,medv)
test = data.frame(lstat,medv)
out_MSE = matrix(0,n,100)
for(j in 1:n){
train_i = train[-j,]
test_i = test[j,]
for(i in 1:100){
near = kknn(medv~lstat,train_i,test_i,k=i,kernel = "rectangular")
aux = mean((test_i[,2]-near$fitted)^2)
out_MSE[j,i] = aux
}
cat(j,'\n')
}
source('~/Desktop/UT MSBA/Predictive Modeling/Section1_MSBA.txt')
library(MASS) ## a library of example datasets
library(class) ## a library with lots of classification tools
library(kknn) ## kknn library
attach(Boston) ## load data file
n = dim(Boston)[1] ## takes in 1st record in dimensions, which is n (sample size)
plot(lstat,medv) ## plots lstat on x axis and medv on y axis
train = data.frame(lstat,medv) ## training dataset
test = data.frame(lstat,medv) ## test dataset, they are the same at this point
ind = order(test[,1]) ## ordering the test data frame for plotting in order
test = test[ind,]
MSE = NULL
kk = c(2,10,50,100,150,200,250,300,400,505) ## various k-nearest neighbors to try
for(i in kk){
near = kknn(medv~lstat,train,test,k=i,kernel = "rectangular") # calculating the k-nearest neighbors
aux = mean((test[,2]-near$fitted)^2) ## calculating mean squared error
MSE = c(MSE,aux) ## setting a list of MSEs for all the different k-nearest neighbors
## c function is concatenating function
plot(lstat,medv,main=paste("k=",i),pch=19,cex=0.8,col="darkgray")
lines(test[,1],near$fitted,col=2,lwd=2) ## connects points in sequence
cat ("Press [enter] to continue") ## break before the next iteration
line <- readline()
}
ii = near$C[i,1:20]
plot(lstat,medv,main=paste("k=",20),pch=19,cex=0.8,col="darkgray")
lines(test[,1],near$fitted,col=2,lwd=2)
abline(v=test[i,1],col=2,lty=2)
points(lstat[ii],medv[ii],pch=19,col="blue")
cat ("Press [enter] to continue")
line <- readline()
######################################
## OUT-OF-SAMPLE Prediction
######################################
train = data.frame(lstat,medv)
test = data.frame(lstat,medv)
tr = sample(1:506,400) ## drawing sample for training set
train = train[tr,]
test = test[-tr,] ## remaining instances for test set
out_MSE = NULL
for(i in 2:350){
near = kknn(medv~lstat,train,test,k=i,kernel = "rectangular")
aux = mean((test[,2]-near$fitted)^2)
out_MSE = c(out_MSE,aux)
}
## set the seed for replication
best = which.min(out_MSE) ## which position of the vector contains the minimum
plot(log(1/(2:350)),sqrt(out_MSE),xlab="Complexity (log(1/k))",ylab="out-of-sample RMSE",col=4,lwd=2,type="l",cex.lab=1.2)
text(log(1/best),sqrt(out_MSE[best])+0.3,paste("k=",best),col=2,cex=1.2)
text(log(1/2),sqrt(out_MSE[2]),paste("k=",2),col=2,cex=1.2)
text(log(1/354)+0.4,sqrt(out_MSE[345]),paste("k=",345),col=2,cex=1.2)
near = kknn(medv~lstat,train,test,k=42,kernel = "rectangular")
ind = order(test[,1])
plot(lstat,medv,main=paste("k=",42),pch=19,cex=0.8,col="darkgray")
lines(test[ind,1],near$fitted[ind],col=2,lwd=2)
?lm
?kknn
?apply
8*6
2^16
2^
5
2^
c(2,3,5,8,13)
Country = c('Brazil', 'China', 'India', 'Switzerland', 'USA')
LifeExpectancy = c(74,76,65,83,79)
Country
LifeExpectancy
Country[1]
seq(0,100,2)
CountryData = data.frame(Country, LifeExpectancy)
CountryData
CountryData%Population = c(199000, 1390000,1240000,7997, 318000)
CountryData$Population = c(199000, 1390000,1240000,7997, 318000)
CountryData
Country = c("Australia", 'Greece')
LifeExpectancy = c(82, 81)
Population = c(23040, 11125)
NewCountryData = data.frame(Country,LifeExpectancy, Population)
NewCountryData
AllCountryData = rbind(CountryData, NewCountryData)
AllCountryData
# Load a toy data and peak at the numbers
data(iris)
# Pick out the first two columns
Z = iris[,c(1,4)]
plot(Z)
Z_centered = scale(Z, center=TRUE, scale=FALSE)
plot(Z_centered)
v_try = rnorm(2)
v_try
v_try = v_try/sqrt(sum(v_try^2))
v_try
sum(v_try^2)
slope = v_try[2]/v_try[1]
plot(Z_centered)
abline(0, slope)
segments(0, 0, v_try[1], v_try[2], col='red', lwd=4)
# Pick some random unit-norm vectors and show the implied subspace
v_try = rnorm(2)
v_try = v_try/sqrt(sum(v_try^2))
slope = v_try[2]/v_try[1]
plot(Z_centered)
abline(0, slope)
segments(0, 0, v_try[1], v_try[2], col='red', lwd=4)
# Pick some random unit-norm vectors and show the implied subspace
v_try = rnorm(2)
v_try = v_try/sqrt(sum(v_try^2))
slope = v_try[2]/v_try[1]
plot(Z_centered)
abline(0, slope)
segments(0, 0, v_try[1], v_try[2], col='red', lwd=4)
# Pick some random unit-norm vectors and show the implied subspace
v_try = rnorm(2)
v_try = v_try/sqrt(sum(v_try^2))
slope = v_try[2]/v_try[1]
plot(Z_centered)
abline(0, slope)
segments(0, 0, v_try[1], v_try[2], col='red', lwd=4)
# Pick some random unit-norm vectors and show the implied subspace
v_try = rnorm(2)
v_try = v_try/sqrt(sum(v_try^2))
slope = v_try[2]/v_try[1]
plot(Z_centered)
abline(0, slope)
segments(0, 0, v_try[1], v_try[2], col='red', lwd=4)
# Pick some random unit-norm vectors and show the implied subspace
v_try = rnorm(2)
v_try = v_try/sqrt(sum(v_try^2))
slope = v_try[2]/v_try[1]
plot(Z_centered)
abline(0, slope)
segments(0, 0, v_try[1], v_try[2], col='red', lwd=4)
# Pick some random unit-norm vectors and show the implied subspace
v_try = rnorm(2)
v_try = v_try/sqrt(sum(v_try^2))
slope = v_try[2]/v_try[1]
plot(Z_centered)
abline(0, slope)
segments(0, 0, v_try[1], v_try[2], col='red', lwd=4)
# Pick some random unit-norm vectors and show the implied subspace
v_try = rnorm(2)
v_try = v_try/sqrt(sum(v_try^2))
slope = v_try[2]/v_try[1]
plot(Z_centered)
abline(0, slope)
segments(0, 0, v_try[1], v_try[2], col='red', lwd=4)
v_try
Z_centered
v_try
Z_centered
v_try
alpha
alpha = Z_centered %*% v_try  # inner product of each row with v_try
alpha
z_hat = alpha %*% v_try  # locations in R^2
points(z_hat, col='blue', pch=4)
segments(0, 0, v_try[1], v_try[2], col='red', lwd=4)
hist(alpha, 25, xlim=c(-3,3), main=round(var(alpha), 2))
# Compare these random projections to the first PC
pc1 = prcomp(Z_centered)
v_best = pc1$rotation[,1]
v_best
slope_best = v_best[2]/v_best[1]  # intercept = 0, slope = rise/run
par(mfrow=c(1,2))
plot(Z_centered, xlim=c(-2.5,2.5), ylim=c(-2.5,2.5))
abline(0, slope_best)  # plot the subspace as a line
alpha_best = Z_centered %*% v_best  # inner product of each row with v_best
z_hat = alpha_best %*% v_best  # locations in R^2
points(z_hat, col='blue', pch=4)
segments(0, 0, v_best[1], v_best[2], col='red', lwd=4)
hist(alpha_best, 25, xlim=c(-3,3), main=round(var(alpha_best), 2))
var(Z_centered[,1])
var(Z_centered[,2])
var(Z_centered[,1]) + var(Z_centered[,2])
pc1 = prcomp(Z_centered)
pc1
names(pc1)
v_best = pc1$rotation[,1]
v_best
set.seed(345)
rnorm(10)
library(gbm)
set.seed(2015)
boost.caravan = gbm(Purchase ~ ., data = Caravan.train, n.trees = 1000, shrinkage = 0.01, distribution = "bernoulli")
library(ISLR)
train = 1:1000
Caravan.train = Caravan[train,]
Caravan.test = Caravan[-train,]
set.seed(2015)
boost.caravan = gbm(Purchase ~ ., data = Caravan.train, n.trees = 1000, shrinkage = 0.01, distribution = "bernoulli")
Caravan$Purchase = ifelse(Caravan$Purchase == "Yes", 1, 0)
Caravan.train = Caravan[train,]
Caravan.test = Caravan[-train,]
train = 1:1000
Caravan$Purchase = ifelse(Caravan$Purchase == "Yes", 1, 0)
Caravan.train = Caravan[train,]
Caravan.test = Caravan[-train,]
library(gbm)
set.seed(2015)
boost.caravan = gbm(Purchase ~ ., data = Caravan.train, n.trees = 1000, shrinkage = 0.01, distribution = "bernoulli")
library(ISLR)
train = 1:1000
Caravan$Purchase = ifelse(Caravan$Purchase == "Yes", 1, 0)
Caravan.train = Caravan[train, ]
Caravan.test = Caravan[-train, ]
library(gbm)
set.seed(342)
boost.caravan = gbm(Purchase ~ ., data = Caravan.train, n.trees = 1000, shrinkage = 0.01, distribution = "bernoulli")
library(ISLR)
train = 1:1000
Caravan$Purchase = ifelse(Caravan$Purchase == "Yes", 1, 0)
Caravan.train = Caravan[train,]
Caravan.test = Caravan[-train,]
library(gbm)
set.seed(2015)
boost.caravan = gbm(Purchase ~ ., data = Caravan.train, n.trees = 1000, shrinkage = 0.01, distribution = "bernoulli")
summary(boost.caravan)
boost.caravan = gbm(Purchase ~ ., data = Caravan.train, n.trees = 1000, shrinkage = 0.01, distribution = "bernoulli")
summary(boost.caravan)
boost.caravan = gbm(Purchase ~ ., data = Caravan.train, n.trees = 1000, shrinkage = 0.01, distribution = "bernoulli")
summary(boost.caravan)
boost.prob = predict(boost.caravan, Caravan.test, n.trees = 1000, type = "response")
boost.pred = ifelse(boost.prob > 0.2, 1, 0)
table(Caravan.test$Purchase, boost.pred)
boost.caravan = gbm(Purchase ~ ., data = Caravan.train, n.trees = 1000, shrinkage = 0.01, distribution = "bernoulli")
summary(boost.caravan)
View(Caravan)
lm.caravan = glm(Purchase ~ ., data = Caravan.train, family = binomial)
lm.prob = predict(lm.caravan, Caravan.test, type = "response")
lm.pred = ifelse(lm.prob > 0.2, 1, 0)
table(Caravan.test$Purchase, lm.pred)
x = model.matrix(crim ~ . - 1, data = Boston)
y = Boston$crim
library(MASS)
library(glmnet)
set.seed(2015)
x = model.matrix(crim ~ . - 1, data = Boston)
y = Boston$crim
cv.lasso = cv.glmnet(x, y, type.measure = "mse")
plot(cv.lasso)
coef(cv.lasso)
sqrt(cv.lasso$cvm[cv.lasso$lambda == cv.lasso$lambda.1se])
names(Boston)
?Boston
x = model.matrix(crim ~ . - 1, data = Boston)
y = Boston$crim
cv.ridge = cv.glmnet(x, y, type.measure = "mse", alpha = 0)
plot(cv.ridge)
coef(cv.ridge)
sqrt(cv.ridge$cvm[cv.ridge$lambda == cv.ridge$lambda.1se])
x = model.matrix(crim ~ . - 1, data = Boston)
y = Boston$crim
cv.ridge = cv.glmnet(x, y, type.measure = "mse", alpha = 0)
plot(cv.ridge)
coef(cv.ridge)
sqrt(cv.ridge$cvm[cv.ridge$lambda == cv.ridge$lambda.1se])
set.seed(2015)
x = model.matrix(crim ~ . - 1, data = Boston)
y = Boston$crim
cv.ridge = cv.glmnet(x, y, type.measure = "mse", alpha = 0)
plot(cv.ridge)
coef(cv.ridge)
sqrt(cv.ridge$cvm[cv.ridge$lambda == cv.ridge$lambda.1se])
set.seed(2015)
x = model.matrix(crim ~ . - 1, data = Boston)
y = Boston$crim
cv.ridge = cv.glmnet(x, y, type.measure = "mse", alpha = 0)
plot(cv.ridge)
coef(cv.ridge)
sqrt(cv.ridge$cvm[cv.ridge$lambda == cv.ridge$lambda.1se])
set.seed(2015)
x = model.matrix(crim ~ . - 1, data = Boston)
y = Boston$crim
cv.ridge = cv.glmnet(x, y, type.measure = "mse", alpha = 0)
plot(cv.ridge)
coef(cv.ridge)
sqrt(cv.ridge$cvm[cv.ridge$lambda == cv.ridge$lambda.1se])
?Boston
set.seed(2015)
x = model.matrix(crim ~ . - 1, data = Boston)
y = Boston$crim
cv.lasso = cv.glmnet(x, y, type.measure = "mse")
plot(cv.lasso)
coef(cv.lasso)
sqrt(cv.lasso$cvm[cv.lasso$lambda == cv.lasso$lambda.1se])
set.seed(2015)
x = model.matrix(crim ~ . - 1, data = Boston)
y = Boston$crim
cv.ridge = cv.glmnet(x, y, type.measure = "mse", alpha = 0)
plot(cv.ridge)
coef(cv.ridge)
sqrt(cv.ridge$cvm[cv.ridge$lambda == cv.ridge$lambda.1se])
setwd("~/Desktop/UT MSBA/Predictive Modeling/mzhang")
beauty <- read.csv("BeautyData.csv", row.names=1)
beauty <- read.csv("BeautyData.csv")
View(beauty)
attach(beauty)
lm.beauty = lm(CourseEvals~BeautyScore)
summary(lm.beauty)
lm.beauty = lm(CourseEvals~., data = beauty)
summary(lm.beauty)
midcity <- read.xls("MidCity.xls")
midcity <- read.csv("MidCity.xls.csv")
setwd("~/Desktop/UT MSBA/Predictive Modeling/mzhang")
midcity <- read.csv("MidCity.xls.csv")
midcity <- read.csv("MidCity.csv")
View(midcity)
attach(midcity)
lm.brick = lm(Price~., data = midcity)
summary(lm.brick)
midcity$nbhdmodern <- ifelse(nbhd == 3, 1, 0)
midcity$nbhdmodern <- ifelse(midcity$nbhd == 3, 1, 0)
midcity$nbhd == 3
unique(midcity$Nbhd)
midcity$nbhdmodern <- ifelse(midcity$Nbhd == 3, 1, 0)
View(midcity)
View(midcity)
midcity.clean <- midcity(, -2)
midcity.clean <- midcity[, -c(2)]
View(midcity.clean)
lm.brick = lm(Price~., data = midcity.clean)
summary(lm.brick)
midcity <- read.csv("MidCity.csv")
attach(midcity)
midcity.clean <- midcity[, -c(2)]
midcity <- read.csv("MidCity.csv")
attach(midcity)
lm.brick = lm(Price~., data = midcity.clean)
summary(lm.brick)
midcity <- midcity[,-c(2)]
midcity <- read.csv("MidCity.csv")
attach(midcity)
midcity <- midcity[,-c(1)]
View(midcity)
lm.brick = lm(Price~., data = midcity.clean)
summary(lm.brick)
lm.brick = lm(Price~., data = midcity)
summary(lm.brick)
midcity$Nbhd <- factor(midcity$Nbhd)
midcity <- read.csv("MidCity.csv")
attach(midcity)
midcity$Nbhd <- factor(midcity$Nbhd)
midcity.clean <- midcity[,-c(1)]
lm.brick = lm(Price~., data = midcity.clean)
summary(lm.brick)
midcity.clean$nbhd3_brickyes <- ifelse(midcity.clean$Nbhd == 3 & midcity.clean$Brick == 'Yes', 1, 0)
View(midcity.clean)
lm.brick_interaction <- lm(Price~., data = midcity.clean)
summary(lm.brick_interaction)
